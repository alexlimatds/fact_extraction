{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "significant-salmon",
   "metadata": {},
   "source": [
    "# Facts extraction with AILA data and LEGAL-BERT-BASE\n",
    "### PyTorch version\n",
    "\n",
    "The model is trained trough a fune-tuning process.\n",
    "\n",
    "The model is evaluated through a cross-validation.\n",
    "\n",
    "We use the train dataset from AILA 2020. This can be obtained at https://github.com/Law-AI/semantic-segmentation;\n",
    "\n",
    "LEGAL-BERT is available at https://huggingface.co/nlpaueb/legal-bert-base-uncased.\n",
    "\n",
    "https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-musical",
   "metadata": {},
   "source": [
    "### Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "racial-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'nlpaueb/legal-bert-base-uncased'\n",
    "model_reference = 'legal-bert-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-westminster",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developing-hormone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eleven-import",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.53.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2021.3.17)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "identical-bahrain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.17.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "agricultural-rebate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0a0+df837d0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sized-sugar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "israeli-disney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.19.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-insurance",
   "metadata": {},
   "source": [
    "### Random numbers' seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "underlying-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-jurisdiction",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "entire-killer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 50\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "train_dir = 'train/'\n",
    "\n",
    "def read_docs(dir_name):\n",
    "  \"\"\"\n",
    "  Read the docs in a directory.\n",
    "  Params:\n",
    "    dir_name : the directory that contains the documents.\n",
    "  Returns:\n",
    "    A dictionary whose keys are the names of the read files and the values are \n",
    "    pandas dataframes. Each dataframe has sentence and label columns.\n",
    "  \"\"\"\n",
    "  docs = {} # key: file name, value: dataframe with sentences and labels\n",
    "  for f in listdir(dir_name):\n",
    "    df = pd.read_csv(\n",
    "        dir_name + f, \n",
    "        sep='\\t', \n",
    "        quoting=csv.QUOTE_NONE, \n",
    "        names=['sentence', 'label'])\n",
    "    docs[f] = df\n",
    "  return docs\n",
    "\n",
    "docs_dic = read_docs(train_dir)\n",
    "\n",
    "print('Number of documents:', len(docs_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "registered-wesley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "\tTrain files: ['d_44.txt', 'd_39.txt', 'd_12.txt', 'd_2.txt', 'd_7.txt', 'd_33.txt', 'd_16.txt', 'd_8.txt', 'd_42.txt', 'd_34.txt', 'd_40.txt', 'd_24.txt', 'd_36.txt', 'd_11.txt', 'd_13.txt', 'd_19.txt', 'd_18.txt', 'd_4.txt', 'd_1.txt', 'd_21.txt', 'd_15.txt', 'd_23.txt', 'd_32.txt', 'd_9.txt', 'd_5.txt', 'd_3.txt', 'd_26.txt', 'd_20.txt', 'd_30.txt', 'd_41.txt', 'd_46.txt', 'd_43.txt', 'd_50.txt', 'd_27.txt', 'd_25.txt', 'd_35.txt', 'd_45.txt', 'd_17.txt', 'd_48.txt', 'd_6.txt']\n",
      "\tTest files: ['d_22.txt', 'd_31.txt', 'd_49.txt', 'd_14.txt', 'd_29.txt', 'd_47.txt', 'd_10.txt', 'd_38.txt', 'd_28.txt', 'd_37.txt']\n",
      "Fold 1:\n",
      "\tTrain files: ['d_22.txt', 'd_31.txt', 'd_49.txt', 'd_14.txt', 'd_29.txt', 'd_47.txt', 'd_10.txt', 'd_38.txt', 'd_28.txt', 'd_37.txt', 'd_40.txt', 'd_24.txt', 'd_36.txt', 'd_11.txt', 'd_13.txt', 'd_19.txt', 'd_18.txt', 'd_4.txt', 'd_1.txt', 'd_21.txt', 'd_15.txt', 'd_23.txt', 'd_32.txt', 'd_9.txt', 'd_5.txt', 'd_3.txt', 'd_26.txt', 'd_20.txt', 'd_30.txt', 'd_41.txt', 'd_46.txt', 'd_43.txt', 'd_50.txt', 'd_27.txt', 'd_25.txt', 'd_35.txt', 'd_45.txt', 'd_17.txt', 'd_48.txt', 'd_6.txt']\n",
      "\tTest files: ['d_44.txt', 'd_39.txt', 'd_12.txt', 'd_2.txt', 'd_7.txt', 'd_33.txt', 'd_16.txt', 'd_8.txt', 'd_42.txt', 'd_34.txt']\n",
      "Fold 2:\n",
      "\tTrain files: ['d_22.txt', 'd_31.txt', 'd_49.txt', 'd_14.txt', 'd_29.txt', 'd_47.txt', 'd_10.txt', 'd_38.txt', 'd_28.txt', 'd_37.txt', 'd_44.txt', 'd_39.txt', 'd_12.txt', 'd_2.txt', 'd_7.txt', 'd_33.txt', 'd_16.txt', 'd_8.txt', 'd_42.txt', 'd_34.txt', 'd_15.txt', 'd_23.txt', 'd_32.txt', 'd_9.txt', 'd_5.txt', 'd_3.txt', 'd_26.txt', 'd_20.txt', 'd_30.txt', 'd_41.txt', 'd_46.txt', 'd_43.txt', 'd_50.txt', 'd_27.txt', 'd_25.txt', 'd_35.txt', 'd_45.txt', 'd_17.txt', 'd_48.txt', 'd_6.txt']\n",
      "\tTest files: ['d_40.txt', 'd_24.txt', 'd_36.txt', 'd_11.txt', 'd_13.txt', 'd_19.txt', 'd_18.txt', 'd_4.txt', 'd_1.txt', 'd_21.txt']\n",
      "Fold 3:\n",
      "\tTrain files: ['d_22.txt', 'd_31.txt', 'd_49.txt', 'd_14.txt', 'd_29.txt', 'd_47.txt', 'd_10.txt', 'd_38.txt', 'd_28.txt', 'd_37.txt', 'd_44.txt', 'd_39.txt', 'd_12.txt', 'd_2.txt', 'd_7.txt', 'd_33.txt', 'd_16.txt', 'd_8.txt', 'd_42.txt', 'd_34.txt', 'd_40.txt', 'd_24.txt', 'd_36.txt', 'd_11.txt', 'd_13.txt', 'd_19.txt', 'd_18.txt', 'd_4.txt', 'd_1.txt', 'd_21.txt', 'd_46.txt', 'd_43.txt', 'd_50.txt', 'd_27.txt', 'd_25.txt', 'd_35.txt', 'd_45.txt', 'd_17.txt', 'd_48.txt', 'd_6.txt']\n",
      "\tTest files: ['d_15.txt', 'd_23.txt', 'd_32.txt', 'd_9.txt', 'd_5.txt', 'd_3.txt', 'd_26.txt', 'd_20.txt', 'd_30.txt', 'd_41.txt']\n",
      "Fold 4:\n",
      "\tTrain files: ['d_22.txt', 'd_31.txt', 'd_49.txt', 'd_14.txt', 'd_29.txt', 'd_47.txt', 'd_10.txt', 'd_38.txt', 'd_28.txt', 'd_37.txt', 'd_44.txt', 'd_39.txt', 'd_12.txt', 'd_2.txt', 'd_7.txt', 'd_33.txt', 'd_16.txt', 'd_8.txt', 'd_42.txt', 'd_34.txt', 'd_40.txt', 'd_24.txt', 'd_36.txt', 'd_11.txt', 'd_13.txt', 'd_19.txt', 'd_18.txt', 'd_4.txt', 'd_1.txt', 'd_21.txt', 'd_15.txt', 'd_23.txt', 'd_32.txt', 'd_9.txt', 'd_5.txt', 'd_3.txt', 'd_26.txt', 'd_20.txt', 'd_30.txt', 'd_41.txt']\n",
      "\tTest files: ['d_46.txt', 'd_43.txt', 'd_50.txt', 'd_27.txt', 'd_25.txt', 'd_35.txt', 'd_45.txt', 'd_17.txt', 'd_48.txt', 'd_6.txt']\n"
     ]
    }
   ],
   "source": [
    "df_folds = pd.read_csv(\n",
    "  'train_docs_by_fold.csv', \n",
    "  sep=';', \n",
    "  names=['fold id', 'train', 'test'], \n",
    "  header=0\n",
    ")\n",
    "\n",
    "train_files_by_fold = {}\n",
    "test_files_by_fold = {}\n",
    "fold_ids = []\n",
    "\n",
    "for _, row in df_folds.iterrows():\n",
    "  fold_id = int(row['fold id'])\n",
    "  fold_ids.append(int(fold_id))\n",
    "  train_files_by_fold[fold_id] = row['train'].split(',')\n",
    "  test_files_by_fold[fold_id] = row['test'].split(',')\n",
    "  print(f'Fold {fold_id}:\\n\\tTrain files: {train_files_by_fold[fold_id]}\\n\\tTest files: {test_files_by_fold[fold_id]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-dynamics",
   "metadata": {},
   "source": [
    "### Tokenizer and Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eastern-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rolled-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "labels_to_idx = {\n",
    "  'Facts' : 0, \n",
    "  'Other' : 1\n",
    "}\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, sentences, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      sentences : list of strings.\n",
    "      lables : list of strings.\n",
    "    \"\"\"\n",
    "    self.len = len(sentences)\n",
    "    self.labels = list(labels)\n",
    "    self.targets = []\n",
    "    for l in labels:\n",
    "      self.targets.append(labels_to_idx[l])\n",
    "    self.targets = torch.tensor(self.targets, dtype=torch.long)\n",
    "    self.data = tokenizer(\n",
    "      sentences, \n",
    "      None,\n",
    "      add_special_tokens=True,\n",
    "      padding='longest', \n",
    "      return_token_type_ids=True, \n",
    "      return_attention_mask=True, \n",
    "      truncation=True, \n",
    "      return_tensors='pt'\n",
    "    )\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return {\n",
    "      'ids': self.data['input_ids'][index],\n",
    "      'mask': self.data['attention_mask'][index], \n",
    "      'token_type_ids': self.data['token_type_ids'][index], \n",
    "      'targets': self.targets[index], \n",
    "      'labels': self.labels[index]\n",
    "    }\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "diagnostic-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(docs_list):\n",
    "  sentences = []\n",
    "  labels = []\n",
    "  for doc_id in docs_list:\n",
    "    sentences.extend(docs_dic[doc_id]['sentence'].to_list())\n",
    "    labels.extend(docs_dic[doc_id]['label'].to_list())\n",
    "  #return MyDataset(sentences[:10], labels[:10], tokenizer) # for code validation\n",
    "  return MyDataset(sentences, labels, tokenizer)\n",
    "\n",
    "def count_labels(set_title, ds):\n",
    "  print(f'{set_title} numbers:')\n",
    "  print(f' Total number of sentences: {ds.len}')\n",
    "  n_facts = len([l for l in ds.labels if l == \"Facts\"])\n",
    "  print(f' Number of Facts labels: {n_facts}')\n",
    "  n_other = len([l for l in ds.labels if l == \"Other\"])\n",
    "  print(f' Number of Other labels: {n_other}')\n",
    "  return n_facts, n_other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-bailey",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eastern-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SentenceClassifier, self).__init__()\n",
    "    self.bert = transformers.AutoModel.from_pretrained(model_id)\n",
    "    self.dropout = torch.nn.Dropout(0.4)\n",
    "    self.classifier = torch.nn.Linear(768, 2) # 768 => hidden vector's dimension, 2 => two classes\n",
    "    torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "    output_1 = self.bert(\n",
    "      input_ids=input_ids,            # input_ids.shape: (batch_size, seq_len)\n",
    "      attention_mask=attention_mask,  # attention_mask.shape: (batch_size, seq_len)\n",
    "      token_type_ids=token_type_ids   # token_type_ids.shape: (batch_size, seq_len)\n",
    "    )\n",
    "    hidden_state = output_1.last_hidden_state # hidden states of last BERT's layer => shape: (batch_size, seq_len, embedd_dim)\n",
    "    cls_embeddings = hidden_state[:, 0]       # hidden states of the CLS tokens => shape: (batch_size, embedd_dim)\n",
    "    cls_embeddings = self.dropout(cls_embeddings)\n",
    "    logits = self.classifier(cls_embeddings)  # shape: (batch_size, 2)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-drinking",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "considered-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model, test_dataloader, loss_function):\n",
    "  predictions = torch.tensor([]).to(device)\n",
    "  y_true = torch.tensor([]).to(device)\n",
    "  eval_loss = 0\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for data in tqdm(test_dataloader, desc='Evaluation'):\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      token_type_ids = data['token_type_ids'].to(device)\n",
    "      y_true_batch = data['targets'].to(device)\n",
    "      y_hat = model.forward(ids, mask, token_type_ids)\n",
    "      loss = loss_function(y_hat, y_true_batch)\n",
    "      eval_loss += loss.item()\n",
    "      predictions_batch = y_hat.argmax(dim=1)\n",
    "      predictions = torch.cat((predictions, predictions_batch))\n",
    "      y_true = torch.cat((y_true, y_true_batch))\n",
    "    predictions = predictions.detach().to('cpu').numpy()\n",
    "    y_true = y_true.detach().to('cpu').numpy()\n",
    "  eval_loss = eval_loss / len(test_dataloader)\n",
    "  # Precision, Recall, F1\n",
    "  t_metrics = precision_recall_fscore_support(\n",
    "    y_true, \n",
    "    predictions, \n",
    "    average='binary', \n",
    "    pos_label=labels_to_idx['Facts'], \n",
    "    zero_division=0)\n",
    "  \n",
    "  return eval_loss, t_metrics[0], t_metrics[1], t_metrics[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-fellow",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "broke-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "\n",
    "supports_by_fold = {}\n",
    "\n",
    "def train(fold_id, learning_rate, weight_decay, n_epochs, batch_size):\n",
    "  ds_train = get_dataset(train_files_by_fold[fold_id])\n",
    "  ds_test = get_dataset(test_files_by_fold[fold_id])\n",
    "  n_facts_train, n_other_train = count_labels('Train dataset', ds_train)\n",
    "  n_facts_test, n_other_test = count_labels('Test dataset', ds_test)\n",
    "  supports_by_fold[fold_id] = {\n",
    "    'facts_train' : n_facts_train, \n",
    "    'other_train' : n_other_train, \n",
    "    'facts_test' : n_facts_test, \n",
    "    'other_test' : n_other_test\n",
    "  }\n",
    "  dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "  dl_test = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  sentence_classifier = SentenceClassifier().to(device)\n",
    "  criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "  optimizer = torch.optim.Adam(\n",
    "    sentence_classifier.parameters(), \n",
    "    lr=learning_rate, \n",
    "    betas=(0.9, 0.999), \n",
    "    eps=1e-8, \n",
    "    weight_decay=weight_decay\n",
    "  )\n",
    "  lr_scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = 0, \n",
    "    num_training_steps = len(dl_train) * n_epochs\n",
    "  )\n",
    "  \n",
    "  metrics = {} # key: epoch number, value: numpy tensor storing train loss, test loss, Precision, Recall, F1\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    sentence_classifier.train()\n",
    "    for train_data in tqdm(dl_train, desc=f'Epoch {epoch} (train)'):\n",
    "      optimizer.zero_grad()\n",
    "      ids = train_data['ids'].to(device)\n",
    "      mask = train_data['mask'].to(device)\n",
    "      token_type_ids = train_data['token_type_ids'].to(device)\n",
    "      y_hat = sentence_classifier(ids, mask, token_type_ids)\n",
    "      y_true = train_data['targets'].to(device)\n",
    "      loss = criterion(y_hat, y_true)\n",
    "      epoch_loss += loss.item()\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(sentence_classifier.parameters(), 1.0)\n",
    "      optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "    epoch_loss = epoch_loss / len(dl_train)\n",
    "    # evaluation\n",
    "    optimizer.zero_grad()\n",
    "    eval_loss, p, r, f1 = evaluate(sentence_classifier, dl_test, criterion)\n",
    "    #storing metrics\n",
    "    metrics[epoch] = np.array([epoch_loss, eval_loss, p, r, f1])\n",
    "    print(f'=> Epoch {epoch}')\n",
    "    print(f'  Train loss: {epoch_loss:.6f}')\n",
    "    print(f'  Test loss:  {eval_loss:.6f}')\n",
    "    print(f'  Precision:  {p:.6f}')\n",
    "    print(f'  Recall:     {r:.6f}')\n",
    "    print(f'  F1:         {f1:.6f}')\n",
    "    time.sleep(0.5) # in order to don't mess the progress bars\n",
    "  \n",
    "  return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "silent-purpose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "               FOLD 0\n",
      "********************************************\n",
      "Train dataset numbers:\n",
      " Total number of sentences: 7693\n",
      " Number of Facts labels: 1584\n",
      " Number of Other labels: 6109\n",
      "Test dataset numbers:\n",
      " Total number of sentences: 1687\n",
      " Number of Facts labels: 635\n",
      " Number of Other labels: 1052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epoch 1 (train): 100%|██████████| 241/241 [03:20<00:00,  1.20it/s]\n",
      "Evaluation: 100%|██████████| 53/53 [00:13<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 1\n",
      "  Train loss: 0.427436\n",
      "  Test loss:  0.655845\n",
      "  Precision:  0.789954\n",
      "  Recall:     0.272441\n",
      "  F1:         0.405152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 (train): 100%|██████████| 241/241 [03:20<00:00,  1.20it/s]\n",
      "Evaluation: 100%|██████████| 53/53 [00:13<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 2\n",
      "  Train loss: 0.337942\n",
      "  Test loss:  0.513950\n",
      "  Precision:  0.715217\n",
      "  Recall:     0.518110\n",
      "  F1:         0.600913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 (train): 100%|██████████| 241/241 [03:20<00:00,  1.20it/s]\n",
      "Evaluation: 100%|██████████| 53/53 [00:13<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 3\n",
      "  Train loss: 0.285297\n",
      "  Test loss:  0.511954\n",
      "  Precision:  0.710775\n",
      "  Recall:     0.592126\n",
      "  F1:         0.646048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 (train): 100%|██████████| 241/241 [03:20<00:00,  1.20it/s]\n",
      "Evaluation: 100%|██████████| 53/53 [00:13<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 4\n",
      "  Train loss: 0.239111\n",
      "  Test loss:  0.554965\n",
      "  Precision:  0.721116\n",
      "  Recall:     0.570079\n",
      "  F1:         0.636763\n",
      "********************************************\n",
      "               FOLD 1\n",
      "********************************************\n",
      "Train dataset numbers:\n",
      " Total number of sentences: 7782\n",
      " Number of Facts labels: 1772\n",
      " Number of Other labels: 6010\n",
      "Test dataset numbers:\n",
      " Total number of sentences: 1598\n",
      " Number of Facts labels: 447\n",
      " Number of Other labels: 1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epoch 1 (train): 100%|██████████| 244/244 [02:55<00:00,  1.39it/s]\n",
      "Evaluation: 100%|██████████| 50/50 [00:14<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 1\n",
      "  Train loss: 0.435274\n",
      "  Test loss:  0.541622\n",
      "  Precision:  0.822430\n",
      "  Recall:     0.393736\n",
      "  F1:         0.532526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 (train): 100%|██████████| 244/244 [02:55<00:00,  1.39it/s]\n",
      "Evaluation: 100%|██████████| 50/50 [00:14<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 2\n",
      "  Train loss: 0.329004\n",
      "  Test loss:  0.516198\n",
      "  Precision:  0.777778\n",
      "  Recall:     0.469799\n",
      "  F1:         0.585774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 (train): 100%|██████████| 244/244 [02:55<00:00,  1.39it/s]\n",
      "Evaluation: 100%|██████████| 50/50 [00:14<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 3\n",
      "  Train loss: 0.272608\n",
      "  Test loss:  0.534407\n",
      "  Precision:  0.739394\n",
      "  Recall:     0.545861\n",
      "  F1:         0.628057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 (train): 100%|██████████| 244/244 [02:55<00:00,  1.39it/s]\n",
      "Evaluation: 100%|██████████| 50/50 [00:14<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 4\n",
      "  Train loss: 0.216023\n",
      "  Test loss:  0.594240\n",
      "  Precision:  0.740385\n",
      "  Recall:     0.516779\n",
      "  F1:         0.608696\n",
      "********************************************\n",
      "               FOLD 2\n",
      "********************************************\n",
      "Train dataset numbers:\n",
      " Total number of sentences: 7002\n",
      " Number of Facts labels: 1844\n",
      " Number of Other labels: 5158\n",
      "Test dataset numbers:\n",
      " Total number of sentences: 2378\n",
      " Number of Facts labels: 375\n",
      " Number of Other labels: 2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epoch 1 (train): 100%|██████████| 219/219 [03:01<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 75/75 [00:16<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 1\n",
      "  Train loss: 0.500213\n",
      "  Test loss:  0.306907\n",
      "  Precision:  0.593407\n",
      "  Recall:     0.576000\n",
      "  F1:         0.584574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 (train): 100%|██████████| 219/219 [03:01<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 75/75 [00:16<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 2\n",
      "  Train loss: 0.358386\n",
      "  Test loss:  0.293164\n",
      "  Precision:  0.630573\n",
      "  Recall:     0.528000\n",
      "  F1:         0.574746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 (train): 100%|██████████| 219/219 [03:01<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 75/75 [00:16<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 3\n",
      "  Train loss: 0.294027\n",
      "  Test loss:  0.317186\n",
      "  Precision:  0.611765\n",
      "  Recall:     0.554667\n",
      "  F1:         0.581818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 (train): 100%|██████████| 219/219 [03:01<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 75/75 [00:16<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 4\n",
      "  Train loss: 0.242084\n",
      "  Test loss:  0.350818\n",
      "  Precision:  0.566351\n",
      "  Recall:     0.637333\n",
      "  F1:         0.599749\n",
      "********************************************\n",
      "               FOLD 3\n",
      "********************************************\n",
      "Train dataset numbers:\n",
      " Total number of sentences: 7581\n",
      " Number of Facts labels: 1863\n",
      " Number of Other labels: 5718\n",
      "Test dataset numbers:\n",
      " Total number of sentences: 1799\n",
      " Number of Facts labels: 356\n",
      " Number of Other labels: 1443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epoch 1 (train): 100%|██████████| 237/237 [03:16<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 57/57 [00:12<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 1\n",
      "  Train loss: 0.487663\n",
      "  Test loss:  0.354011\n",
      "  Precision:  0.635945\n",
      "  Recall:     0.387640\n",
      "  F1:         0.481675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 (train): 100%|██████████| 237/237 [03:16<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 57/57 [00:12<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 2\n",
      "  Train loss: 0.378554\n",
      "  Test loss:  0.393379\n",
      "  Precision:  0.566416\n",
      "  Recall:     0.634831\n",
      "  F1:         0.598675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 (train): 100%|██████████| 237/237 [03:16<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 57/57 [00:12<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 3\n",
      "  Train loss: 0.313340\n",
      "  Test loss:  0.408837\n",
      "  Precision:  0.534653\n",
      "  Recall:     0.758427\n",
      "  F1:         0.627178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 (train): 100%|██████████| 237/237 [03:16<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 57/57 [00:12<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 4\n",
      "  Train loss: 0.256533\n",
      "  Test loss:  0.405952\n",
      "  Precision:  0.562780\n",
      "  Recall:     0.705056\n",
      "  F1:         0.625935\n",
      "********************************************\n",
      "               FOLD 4\n",
      "********************************************\n",
      "Train dataset numbers:\n",
      " Total number of sentences: 7462\n",
      " Number of Facts labels: 1813\n",
      " Number of Other labels: 5649\n",
      "Test dataset numbers:\n",
      " Total number of sentences: 1918\n",
      " Number of Facts labels: 406\n",
      " Number of Other labels: 1512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epoch 1 (train): 100%|██████████| 234/234 [03:13<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 60/60 [00:09<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 1\n",
      "  Train loss: 0.517244\n",
      "  Test loss:  0.356118\n",
      "  Precision:  0.768657\n",
      "  Recall:     0.253695\n",
      "  F1:         0.381481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 (train): 100%|██████████| 234/234 [03:13<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 60/60 [00:09<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 2\n",
      "  Train loss: 0.399296\n",
      "  Test loss:  0.331965\n",
      "  Precision:  0.650273\n",
      "  Recall:     0.586207\n",
      "  F1:         0.616580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 (train): 100%|██████████| 234/234 [03:13<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 60/60 [00:09<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 3\n",
      "  Train loss: 0.339074\n",
      "  Test loss:  0.370021\n",
      "  Precision:  0.592742\n",
      "  Recall:     0.724138\n",
      "  F1:         0.651885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 (train): 100%|██████████| 234/234 [03:13<00:00,  1.21it/s]\n",
      "Evaluation: 100%|██████████| 60/60 [00:09<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 4\n",
      "  Train loss: 0.293725\n",
      "  Test loss:  0.359000\n",
      "  Precision:  0.623853\n",
      "  Recall:     0.669951\n",
      "  F1:         0.646081\n",
      "CPU times: user 51min, sys: 16min 57s, total: 1h 7min 58s\n",
      "Wall time: 1h 8min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training params\n",
    "n_epochs = 4\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 1e-3\n",
    "batch_size = 32\n",
    "\n",
    "#fold_ids = [0, 1] # for code validation\n",
    "raw_metrics = {} # key: epoch, value: numpy tensor of shape (n_folds, 5)\n",
    "for fold_id in fold_ids:\n",
    "  print('********************************************')\n",
    "  print(f'               FOLD {fold_id}')\n",
    "  print('********************************************')\n",
    "  fold_metrics = train(fold_id, learning_rate, weight_decay, n_epochs, batch_size)\n",
    "  for epoch, scores in fold_metrics.items():\n",
    "    epoch_metrics = raw_metrics.get(epoch, None)\n",
    "    if epoch_metrics is None:\n",
    "      raw_metrics[epoch] = scores\n",
    "    else:\n",
    "      raw_metrics[epoch] = np.vstack((epoch_metrics, scores))\n",
    "\n",
    "metrics = pd.DataFrame(columns=['Epoch', 'Train loss', 'std', 'Test loss', 'std', 'Precision', 'P std', 'Recall', 'R std', 'F1', 'F1 std'])\n",
    "for i, (epoch, scores) in enumerate(raw_metrics.items()):\n",
    "  mean = np.mean(scores, axis=0)\n",
    "  std = np.std(scores, axis=0)\n",
    "  metrics.loc[i] = [\n",
    "      f'{epoch}', \n",
    "      f'{mean[0]:.6f}', f'{std[0]:.6f}',  # train loss\n",
    "      f'{mean[1]:.6f}', f'{std[1]:.6f}',  # test loss\n",
    "      f'{mean[2]:.4f}', f'{std[2]:.4f}',  # precision\n",
    "      f'{mean[3]:.4f}', f'{std[3]:.4f}',  # recall\n",
    "      f'{mean[4]:.4f}', f'{std[4]:.4f}'   # f1\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-training",
   "metadata": {},
   "source": [
    "### Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "rental-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_report(avg_metrics, complete_metrics, dest_dir):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "    avg_metrics : A pandas Dataframe with the averaged metrics.\n",
    "    complete_metrics : A dictionary with the metrics by epoch. The key indicates the epoch. \n",
    "              Each value must be a numpy tensor of shape (n_folds, 5).\n",
    "    dest_dir : The directory where the report will be saved.\n",
    "  \"\"\"\n",
    "  report = (\n",
    "      'RESULTS REPORT\\n'\n",
    "      'Model: Legal BERT\\n'\n",
    "      'Evaluation: cross-validation\\n'\n",
    "      'Train scheme: fine-tuning\\n'\n",
    "      f'Batch size: {batch_size}\\n'\n",
    "      f'Learning rate: {learning_rate}\\n'\n",
    "      f'Weight decay: {weight_decay}\\n\\n'\n",
    "  )\n",
    "  \n",
    "  report += 'Averages:\\n'\n",
    "  report += avg_metrics.to_string(index=False, justify='center')\n",
    "  \n",
    "  report += '\\n\\nDetailed report:\\n'\n",
    "  report += 'Supports by fold:\\n'\n",
    "  for fold_id, support in supports_by_fold.items():\n",
    "    report += f'=> Fold {fold_id}:\\n'\n",
    "    report += f'  Facts (train): {support[\"facts_train\"]}\\t\\tOther (train): {support[\"other_train\"]}\\n'\n",
    "    report += f'  Facts (test): {support[\"facts_test\"]}\\t\\tOther (test): {support[\"other_test\"]}\\n'\n",
    "  \n",
    "  report += '\\nScores:\\n'\n",
    "  for epoch, scores in complete_metrics.items():\n",
    "    df = pd.DataFrame(\n",
    "      scores, \n",
    "      columns=['Train loss', 'Test loss', 'Precision', 'Recall', 'F1'], \n",
    "      index=[f'Fold {i}' for i in range(scores.shape[0])])\n",
    "    report += f'Epoch: {epoch}\\n' + df.to_string() + '\\n\\n'\n",
    "    \n",
    "  with open(dest_dir + f'report-{model_reference}_ft_{datetime.now().strftime(\"%Y-%m-%d-%Hh%Mmin\")}.txt', 'w') as f:\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abstract-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_report(metrics, raw_metrics, './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "latter-example",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>std</th>\n",
       "      <th>Test loss</th>\n",
       "      <th>std</th>\n",
       "      <th>Precision</th>\n",
       "      <th>P std</th>\n",
       "      <th>Recall</th>\n",
       "      <th>R std</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.473566</td>\n",
       "      <td>0.035807</td>\n",
       "      <td>0.442900</td>\n",
       "      <td>0.133430</td>\n",
       "      <td>0.7221</td>\n",
       "      <td>0.0904</td>\n",
       "      <td>0.3767</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.4771</td>\n",
       "      <td>0.0761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.360637</td>\n",
       "      <td>0.025831</td>\n",
       "      <td>0.409731</td>\n",
       "      <td>0.091760</td>\n",
       "      <td>0.6681</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>0.5474</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>0.5953</td>\n",
       "      <td>0.0142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.300869</td>\n",
       "      <td>0.023252</td>\n",
       "      <td>0.428481</td>\n",
       "      <td>0.082919</td>\n",
       "      <td>0.6379</td>\n",
       "      <td>0.0761</td>\n",
       "      <td>0.6350</td>\n",
       "      <td>0.0888</td>\n",
       "      <td>0.6270</td>\n",
       "      <td>0.0246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.249495</td>\n",
       "      <td>0.025647</td>\n",
       "      <td>0.452995</td>\n",
       "      <td>0.101820</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.6198</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.6234</td>\n",
       "      <td>0.0172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Epoch Train loss       std Test loss       std Precision   P std  Recall  \\\n",
       "0     1   0.473566  0.035807  0.442900  0.133430    0.7221  0.0904  0.3767   \n",
       "1     2   0.360637  0.025831  0.409731  0.091760    0.6681  0.0725  0.5474   \n",
       "2     3   0.300869  0.023252  0.428481  0.082919    0.6379  0.0761  0.6350   \n",
       "3     4   0.249495  0.025647  0.452995  0.101820    0.6429  0.0752  0.6198   \n",
       "\n",
       "    R std      F1  F1 std  \n",
       "0  0.1150  0.4771  0.0761  \n",
       "1  0.0573  0.5953  0.0142  \n",
       "2  0.0888  0.6270  0.0246  \n",
       "3  0.0681  0.6234  0.0172  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "metrics_display = display(metrics, display_id='metrics_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-balance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
