{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FACTS AILA data augmentation mixup SBERT-LaBSE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOICruZDVy5jSOJfNasgcGV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlimatds/fact_extraction/blob/main/AILA2020/FACTS_AILA_data_augmentation_mixup_SBERT_LaBSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmlE7Ou2rBxU"
      },
      "source": [
        "### Mixup Data Augmentation\n",
        "\n",
        "In this notebook we exploit the Mixup data augmentation approach to create additional data using AILA dataset as source. Care was taken to use two vectors from different classes when creating a new agumented vector.\n",
        "\n",
        "The feature vectors are created with a SBERT/LaBSE model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Notebook parameters"
      ],
      "metadata": {
        "id": "J3vMd_67ml9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'sentence-transformers/LaBSE'\n",
        "model_reference = 'SBERT-LaBSE'"
      ],
      "metadata": {
        "id": "ZselZfA5mo4y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing dependencies"
      ],
      "metadata": {
        "id": "6I6gy7LS1r2O"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI2nNhaBq69v",
        "outputId": "9bdc85d9-be80-4f8c-bb8d-fdce48e3eab7"
      },
      "source": [
        "pip install -U sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.18.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading dataset"
      ],
      "metadata": {
        "id": "Hno64doH1190"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO3h7fDord-0",
        "outputId": "e281fd53-1b78-43a0-917c-9eec146b672c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "g_drive_dir = '/content/gdrive/MyDrive/'\n",
        "dataset_dir = 'fact_extraction_AILA/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk1_dghLrerY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ceb6468-da74-489b-c928-f52f10575291"
      },
      "source": [
        "!mkdir data\n",
        "!mkdir data/train\n",
        "!tar -xf {g_drive_dir}{dataset_dir}train.tar.xz -C data/train\n",
        "\n",
        "train_dir = 'data/train/'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘data/train’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5HU1tyTroQN",
        "outputId": "40f7c904-e0f0-42cc-e948-87838398bdd3"
      },
      "source": [
        "import pandas as pd\n",
        "from os import listdir\n",
        "import csv\n",
        "\n",
        "def read_docs(dir_name):\n",
        "  \"\"\"\n",
        "  Read the docs in a directory.\n",
        "  Params:\n",
        "    dir_name : the directory that contains the documents.\n",
        "  Returns:\n",
        "    A dictionary whose keys are the names of the read files and the values are \n",
        "    pandas dataframes. Each dataframe has the sentence and label columns.\n",
        "  \"\"\"\n",
        "  docs = {} # key: file name, value: dataframe with sentences and labels\n",
        "  for f in listdir(dir_name):\n",
        "    df = pd.read_csv(\n",
        "        dir_name + f, \n",
        "        sep='\\t', \n",
        "        quoting=csv.QUOTE_NONE, \n",
        "        names=['sentence', 'label'])\n",
        "    docs[f] = df\n",
        "  return docs\n",
        "\n",
        "docs_train = read_docs(train_dir)\n",
        "\n",
        "print(f'TRAIN: {len(docs_train)} documents read.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: 50 documents read.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spliting documents according to folds"
      ],
      "metadata": {
        "id": "-H9C472k7vmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the file containing the sets of trains documents and test documents by fold\n",
        "train_files_by_fold = {}  # Key: fold ID, value: file names (list of string)\n",
        "\n",
        "df_folds = pd.read_csv(\n",
        "  g_drive_dir + dataset_dir + 'train_docs_by_fold.csv', \n",
        "  sep=';', \n",
        "  names=['fold id', 'train', 'test'], \n",
        "  header=0)\n",
        "for idx, row in df_folds.iterrows():\n",
        "  train_files_by_fold[row['fold id']] = row['train'].split(',')\n"
      ],
      "metadata": {
        "id": "qkOYbvrW76Cd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SBERT model"
      ],
      "metadata": {
        "id": "F3NcNYjM1-Z_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NZCKEmbrs5u"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sent_encoder = SentenceTransformer(model_id)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoding sentences"
      ],
      "metadata": {
        "id": "CRi9OWY8CXUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_docs_train = {} # key: document ID, value: encoded sentences (PyTorch matrix)\n",
        "for doc_id, doc_df in docs_train.items():\n",
        "  encoded_docs_train[doc_id] = sent_encoder.encode(doc_df['sentence'].to_list(), convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "cnnXdmHtCX1n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data augmentation functions"
      ],
      "metadata": {
        "id": "A5vLfMSR2Fre"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paS01kOi09O2"
      },
      "source": [
        "import numpy as np\n",
        "from torch.distributions.beta import Beta\n",
        "\n",
        "def mixup(xi, xj, yi, yj, alpha):\n",
        "  \"\"\"\n",
        "  Mixup function: generates an synthetic vector from two source vectors. For details, \n",
        "  check the mixup paper.\n",
        "  Arguments:\n",
        "    xi : the first source vector.\n",
        "    xj : the second source vector.\n",
        "    yi : the hot-one-encoded label vector of the first source vector.\n",
        "    yj : the hot-one-encoded label vector of the second source vector.\n",
        "    alpha : hyperparameter of the beta distribution to be used to generate the lambda value.\n",
        "  Returns:\n",
        "    The generated synthetic vector.\n",
        "    The label vector of the generated synthetic vector.\n",
        "  \"\"\"\n",
        "  # TODO: generate a tensor of lambdas\n",
        "  #lam = np.random.beta(alpha, alpha)\n",
        "  b = Beta(alpha, alpha)\n",
        "  lam = b.rsample(sample_shape=(xi.shape[0], 1))\n",
        "  lam_x = lam.broadcast_to(xi.shape).to(xi.device)\n",
        "  #x_hat = lam * xi + (1 - lam) * xj\n",
        "  x_hat = lam_x * xi + (1 - lam_x) * xj\n",
        "  lam_y = lam.broadcast_to(yi.shape).to(yi.device)\n",
        "  #y_hat = lam * yi + (1 - lam) * yj\n",
        "  y_hat = lam_y * yi + (1 - lam_y) * yj\n",
        "  return x_hat, y_hat"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "def data_by_class(doc_id_list):\n",
        "  \"\"\"\n",
        "  Returns, grouped per class, the vector embeddings of the sentences in a set of \n",
        "  documents.\n",
        "  Params:\n",
        "    A list of document IDs.\n",
        "  Returns:\n",
        "    The embeddings of the Facts class (PyTorch matrix).\n",
        "    The embeddings of the Other class (PyTorch matrix).\n",
        "  \"\"\"\n",
        "  sent_embeddings = None\n",
        "  labels = None\n",
        "  for doc_id in doc_id_list:\n",
        "    if sent_embeddings is None:\n",
        "      sent_embeddings = encoded_docs_train[doc_id]\n",
        "      labels = docs_train[doc_id]['label'].to_numpy()\n",
        "    else:\n",
        "      sent_embeddings = torch.vstack((sent_embeddings, encoded_docs_train[doc_id]))\n",
        "      labels = np.concatenate((labels, docs_train[doc_id]['label'].to_numpy()))\n",
        "  \n",
        "  facts_idx = np.nonzero(labels == 'Facts')[0]\n",
        "  facts_embeddings = sent_embeddings[facts_idx,:]\n",
        "  other_idx = np.nonzero(labels == 'Other')[0]\n",
        "  other_embeddings = sent_embeddings[other_idx,:]\n",
        "\n",
        "  return facts_embeddings, other_embeddings\n",
        "\n",
        "def augment_data(alpha, doc_id_list):\n",
        "  \"\"\"\n",
        "  Generates a set of synthetic embedding vectors from the sentences in a provided set of \n",
        "  documents. The sentences are selected at random. It uses sentences of different \n",
        "  classes to generate a synthetic vector.\n",
        "  Params:\n",
        "    alpha: hyperparameter of the beta distribution to be used with the mixup algorithm.\n",
        "    doc_id_list: a list with the IDs of the source documents (list of strings).\n",
        "  Returns:\n",
        "    The generated feature vectors (PyTorch tensor).\n",
        "    The generated target vectors (PyTorch tensor).\n",
        "  \"\"\"\n",
        "  N_synthetic = 3500 # number of synthetic vectors to be generated\n",
        "  facts_embeddings, other_embeddings = data_by_class(doc_id_list)\n",
        "  # random indexes for the Facts class\n",
        "  idx_i = random.choices(range(facts_embeddings.shape[0]), k=N_synthetic)\n",
        "  # random indexes for the Other class\n",
        "  idx_j = random.choices(range(other_embeddings.shape[0]), k=N_synthetic)\n",
        "  # getting source vectors to generate the augmented vectors\n",
        "  x_i = facts_embeddings[idx_i, :]\n",
        "  x_j = other_embeddings[idx_j, :]\n",
        "  y_i = torch.zeros(x_i.shape[0], 2)\n",
        "  y_i[:, 0] = 1   # targets of the Facts class: [1, 0]\n",
        "  y_j = torch.zeros(x_j.shape[0], 2)\n",
        "  y_j[:, 1] = 1  # targets of the Other class: [0, 1]\n",
        "  # data augmentation\n",
        "  X_aug, Y_aug = mixup(x_i, x_j, y_i, y_j, alpha)\n",
        "\n",
        "  return X_aug, Y_aug\n"
      ],
      "metadata": {
        "id": "ggnnmCQdqi0b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OkYtJ6eCdrX"
      },
      "source": [
        "#### Generating and writing the augmented data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def write_vectors(X_hat, Y_hat, file_prefix):\n",
        "  \"\"\"\n",
        "  Saves a set of synthetic vectors. The vectors are saved as numpy data.\n",
        "  Arguments:\n",
        "    X_hat : the set of synthetic vectors (PyTorch matrix).\n",
        "    Y_hat : the vector labels of the X_hat vectors (PyTorch matrix).\n",
        "    file_prefix : the prefix of the generated files' names.\n",
        "  \"\"\"\n",
        "  output_dir = f'{g_drive_dir}{dataset_dir}mixup_data_{model_reference}/'\n",
        "  np.save(output_dir + file_prefix + '_features.npy', X_hat.detach().cpu())\n",
        "  np.save(output_dir + file_prefix + '_targets.npy', Y_hat.detach().cpu())\n",
        "\n",
        "alphas = [0.1, 0.5, 1.0, 4.0]"
      ],
      "metadata": {
        "id": "xpvvp7p8kdpi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating augmented data with the hole train set\n",
        "for a in alphas:\n",
        "  X_hat, Y_hat = augment_data(a, docs_train.keys())\n",
        "  file_prefix = f'alpha_{str(a).replace(\".\", \"_\")}'\n",
        "  write_vectors(X_hat, Y_hat, file_prefix)"
      ],
      "metadata": {
        "id": "ff9Vor93nYUK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating augmented data for cross-validation\n",
        "for a in alphas:\n",
        "  for fold_id, doc_ids in train_files_by_fold.items():\n",
        "    X_hat, Y_hat = augment_data(a, doc_ids)\n",
        "    file_prefix = f'alpha_{str(a).replace(\".\", \"_\")}_fold_{fold_id}'\n",
        "    write_vectors(X_hat, Y_hat, file_prefix)"
      ],
      "metadata": {
        "id": "jEUMtAtOnik1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### References\n",
        "\n",
        "- Paheli Bhattacharya, Shounak Paul, Kripabandhu Ghosh, Saptarshi Ghosh, and Adam Wyner. 2019. **Identification of Rhetorical Roles of Sentences in Indian Legal Judgments**. In Proc. International Conference on Legal Knowledge and Information Systems (JURIX).\n",
        "- Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, David Lopez-Paz: **mixup: Beyond Empirical Risk Minimization**. ICLR (Poster) 2018"
      ],
      "metadata": {
        "id": "1wVIU3iijihY"
      }
    }
  ]
}